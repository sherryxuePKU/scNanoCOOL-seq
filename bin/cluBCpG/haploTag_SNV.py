# from clubcpg.ParseBam import BamFileReadParser
# from clubcpg.ParseBam_NOMe_indel_snv import BamFileReadParser_NOMe_indel_snv
from clubcpg.ParseBam_haplotag import HaploTag
import os
import logging
from multiprocessing import Pool
import numpy as np
from collections import defaultdict
import time
from pandas.core.indexes.base import InvalidIndexError
from collections import defaultdict
from functools import reduce
import re
import pysam
import pandas as pd
from datetime import datetime


class haploTagging:
    """
    Usage: to find the candidate region of passive demethylation

    Testing:
        >>> from clubcpg.haploTag_SNV import haploTagging
        >>> input_bam_file = "cool_P31_21.clean_bismark_mm2.sort.rmdup.CHlt80per.bam"
        >>> outdir = "/gpfs1/tangfuchou_pkuhpc/tangfuchou_coe/xuexiaohui/project/nanoCool/DNA/02.bam/cool_P31_21"
        >>> chdir(outdir)
        >>> test = haploTagging(input_bam_file, "test_SNP.tsv", outdir, 2)
        >>> test_res = test.analyze_snps()
    
    """
    def __init__(self,
                 bam_file, snp_file, output_directory, number_of_processors=2):
        """
        This class is initialized with a path to a bam file and a bin size
    
        :param bam_file: BAM file to be haplotagged
        :param snp_file: TSV file generated by SNPsplit (ID, chr, pos, value, genotype)
        :param output_name: filename of output
        :number_of_processors: How many CPUs to use for parallel computation, default=1
        """
        self.input_bam_file = bam_file
        self.input_snp_file = snp_file
        self.outdir = output_directory
        self.number_of_processors = int(number_of_processors)
        self.snp_res = []

    def readSNPs(self, snp):
        """
        Take a single snp, return two lists: qname of paternal reads & qname of maternal reads. 
        This is passed to a multiprocessing Pool.

        :param snp: SNP should be passed as "Chr19_4343343_A/G"
        :return: p.list=[], m.list=[]
        """
        # Get reads from bam file
        parser = HaploTag(self.input_bam_file)

        # Split bin into parts
        chr, snp_loc, genotype = snp.split("_")
        ref_base = genotype.split("/")[0]
        snp_base = genotype.split("/")[1]
        snp_loc = int(snp_loc)
        try:
            reads = parser.parse_reads(chr, snp_loc, ref_base, snp_base)
            # matrix_a = parser_a.create_matrix(reads_a)
            # matrix_a = matrix_a.dropna(how="all")
        except BaseException as e:
            # No reads are within this window, do nothing
            # self.bins_no_reads += 1
            return None
        except:
            logging.error("Unknown error: {}".format(snp))
            return None

        # drop rows of ALL NaN
        # matrix = matrix.dropna(how="all")
        # convert to data_frame of 1s and 0s, drop rows with NaN
        # matrix = matrix.dropna()

        return reads
    
    def assign_read(self, qn):
        ref_qname = []
        alt_qname = []
            
        m = [i[1] for i in self.snp_res if re.match(qn,i[0])]
        g1 = m.count('genome1')
        g2 = m.count('genome2')
        # gt_count = str(g1) + "/" + str(g2)
        # qn_dict[qn].extend(gt_count)
        if g1!=0:
            if g2==0: ref_qname.append(qn)
            elif g1/g2 >= 2: ref_qname.append(qn)
        if g2!=0:
            if g1==0: alt_qname.append(qn)
            elif g2/g1 >= 2: alt_qname.append(qn)
            
        return ref_qname, alt_qname

    def get_chromosome_lengths(self):
        """
        Get dictionary containing lengths of the chromosomes. Uses bam file for reference

        :return: Dictionary of chromosome lengths, ex: {"chrX": 222222}
        """
        parser = HaploTag(self.input_bam_file)
        return dict(zip(parser.OpenBamFile.references, parser.OpenBamFile.lengths))

    @staticmethod
    def remove_scaffolds(chromosome_len_dict):
        """
        Return a dict containing only the standard chromosomes starting with "chr"

        :param chromosome_len_dict: A dict generated by get_chromosome_lenghts()
        :return: a dict containing only chromosomes starting with "chr"
        """
        new_dict = dict(chromosome_len_dict)
        for key in chromosome_len_dict.keys():
            if not key.startswith("chr"):
                new_dict.pop(key)

        return new_dict

    # def generate_snps_list(self, chromosome_len_dict):
    #     """
    #     Get a dict of lists of all snps for all chromosomes in the passed dict

    #     :param chromosome_len_dict: A dict of chromosome length sizes from get_chromosome_lenghts, 
    #                                 cleaned up by remove_scaffolds() if desired
    #     :return: dict with each key being a chromosome. ex: chr1
    #     """
    #     all_snps = defaultdict(list)
    #     for key, value in chromosome_len_dict.items():
    #         bins = list(np.arange(self.pos, value + self.bin_size, self.bin_size))
    #         bins = ["_".join([key, str(x)]) for x in bins]
    #         all_bins[key].extend(bins)

    #     return all_bins
    # @staticmethod
    def extract_first(self, lst, coord: int):
        return reduce(lambda acc, x: acc + [x.split("\t")[coord]] if x else acc, lst, [])

    def generate_snps_list(self):
        """
        Get a dict of lists of all snps for all chromosomes
        
        :param snp_file: A TSV file generated by SNPsplit
        e.g. 46960992        11(chr)      18529780(pos)        1       T/C(ref/snp)
        :return: dict with each key being a chromosome, ex: 1
        """
        # snp_f = open(self.input_snp_file, "r")
        # lines = snp_f.readlines()
        # snp_f.close()

        ## Optimized for acceleration
        ## 230602
        # with open(self.input_snp_file, 'r') as snp_f:
        #     lines = snp_f.read().splitlines()
            
        # ## get the list of chr, pos, and gt
        # chromosomes = self.extract_first(lines, 1)
        # positions = self.extract_first(lines, 2)
        # genotypes = self.extract_first(lines, 4)

        # ## concatenate the lists into a single list
        # snps = [str(chr) + "_"  + str(pos) + "_" + str(gt) for chr, pos, gt in zip(chromosomes, positions, genotypes) ]
        # # snps = "chr" + snps
        # snps = ["".join(["chr", str(x)]) for x in snps]
        
        '''
        added 230602 to accelerate
            1) use pandas to read snp_file;
            2) use assign to paste columns;
            ref: https://stackoverflow.com/questions/11858472/string-concatenation-of-two-pandas-columns
        '''
        # snp_f = pd.read_csv(self.input_snp_file, sep = "\t")
        # snp_f.columns = ['id', 'chr', 'pos', 'snp_value', 'genotype']
        # add 230603, column chr contains number and character X & Y
        colnames_dict = {'id': str, 'chr': str, 'pos': int, 'snp_value': int, 'genotype': str}
        snp_f = pd.read_csv(self.input_snp_file, sep = "\t", engine='python', header=None, names = list(colnames_dict.keys()), dtype = colnames_dict)
        snp_f = snp_f.astype('str')
        snp_f = snp_f.assign(snps="chr" + snp_f.chr + "_" + snp_f.pos + "_" + snp_f.genotype)
        snps = list(snp_f.snps)

        ## get unique chrs as keys
        # chromosomes = list(set(chromosomes))
        chromosomes = list(set(list(snp_f.snps)))
        # all_snps = defaultdict(list)

        # for key in chromosomes:
        #     it = re.finditer(str(key) + "_", snps)
        #     chr_snps = []
        #     for m in it:
        #         chr_snps.append(snps[m.start()])
        #     all_snps[key].extend(chr_snps)
        
        # return all_snps
        return snps

    def extract_reads(self, qname_list, hp):
        # n = get_names(options.names)
        bamfile = pysam.AlignmentFile(self.input_bam_file, 'rb')
        name_indexed = pysam.IndexedReads(bamfile)
        name_indexed.build()
        header = bamfile.header.copy()

        outfile=os.path.basename(self.input_bam_file)
        output_file = os.path.join(self.outdir, "{}.haplotagg_{}.bam".format(outfile, hp))
        out = pysam.Samfile(output_file, 'wb', header=header)

        for qn in qname_list:
            try:
                name_indexed.find(qn)
            except KeyError:
                pass
            else:
                iterator = name_indexed.find(qn)
                for x in iterator:
                    out.write(x)
        # return len(qname_list)


    def analyze_snps(self, individual_chrom=None):
        """
        Main function in class. Run the Complete analysis on the data

        :param individual_chrom: Chromosome to analyze: ie "7"
        :return: filename of the generated report
        """

        # Track the progress of the multiprocessing and output
        def track_progress(job, update_interval=60):
            while job._number_left > 0:
                logging.info("Tasks remaining = {0}".format(
                    job._number_left * job._chunksize))
                time.sleep(update_interval)

        # Get and clean dict of chromosome lenghts, convert to list of bins
        # chromosome_lengths = self.get_chromosome_lengths()
        # chromosome_lengths = self.remove_scaffolds(chromosome_lengths)

        # If one chromosome was specified use only that chromosome
        # if individual_chrom:
        #     new = dict()
        #     new[individual_chrom] = chromosome_lengths[individual_chrom]
        #     chromosome_lengths = new
        readsnp_start = datetime.now()
        logging.info("{}:Read snp_file begins!".format(readsnp_start.strftime('%Y-%m-%d %H:%M:%S')))        

        snps_to_analyze = self.generate_snps_list()

        readsnp_end = datetime.now()
        logging.info("{}:Read snp_file is complete!".format(readsnp_end.strftime('%Y-%m-%d %H:%M:%S')))

        readsnp_delta =  readsnp_end - readsnp_start
        readsnp_delta = readsnp_delta.total_seconds()
        logging.info("Read snp_file takes {} seconds!".format(str(readsnp_delta)))

        # Set up for multiprocessing
        assginsnp_start = datetime.now()
        logging.info("{}:SNP assignment begins!".format(assginsnp_start.strftime('%Y-%m-%d %H:%M:%S')))  

        final_results = []
        # for key in snps_to_analyze.keys():
        # modified in 230603, to close pool
        # pool = Pool(processes=self.number_of_processors)
        with Pool(processes=self.number_of_processors) as pool:
            # results = pool.map_async(self.readSNPs, snps_to_analyze[key])
            results = pool.map_async(self.readSNPs, snps_to_analyze)
            track_progress(results)

            results = results.get()
            final_results.extend(results)

        assginsnp_end = datetime.now()
        logging.info("{}:SNP assignment is complete!".format(assginsnp_end.strftime('%Y-%m-%d %H:%M:%S')))

        assginsnp_delta = assginsnp_end - assginsnp_start
        assginsnp_delta = assginsnp_delta.total_seconds()
        logging.info("SNP assignment takes {} seconds!".format(str(assginsnp_delta)))

        ## Summarize hp_tags by read
        tagread_start = datetime.now()
        logging.info("{}:Read assignment begins!".format(tagread_start.strftime('%Y-%m-%d %H:%M:%S')))

        # outfile=os.path.basename(self.input_bam_file)
        # output_file = os.path.join(self.output_directory, "CompleteSNPs.{}.{}.csv".format(outfile, individual_chrom))

        # added 230601
        # added 230602, remove snps not covered, result in None in list
        # flat_res = [item for sublist in final_results for item in sublist]

        ## these two steps are very time consuming!!
        flat_res = [item for sublist in final_results if sublist is not None for item in sublist]
        self.snp_res = flat_res

        # qnames = reduce(lambda acc, x: acc + [x[0]] if x else acc, flat_res, [])
        # qnames = list(set(qnames))
        # added in 230605, to save time
        bamfile = pysam.AlignmentFile(self.input_bam_file, 'rb')
        full_reads = bamfile.fetch()
        full_qnames = [i.query_name for i in full_reads]

        '''
        Read-level haplotype assignment strategy:
            1) except for "unassigned", all the same hp_tag;
            2) have both "genome1" and "genome2", one of them should be at least twice of the other

            reference: https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001017
        '''
        # ref_qname = []
        # alt_qname = []
        
        # for qn in qnames:
        #     m = [i[1] for i in flat_res if re.match(qn,i[0])]
        #     g1 = m.count('genome1')
        #     g2 = m.count('genome2')
        #     # gt_count = str(g1) + "/" + str(g2)
        #     # qn_dict[qn].extend(gt_count)
        #     if g1!=0:
        #         if g2==0: ref_qname.append(qn)
        #         elif g1/g2 >= 2: ref_qname.append(qn)
        #     if g2!=0:
        #         if g1==0: alt_qname.append(qn)
        #         elif g2/g1 >= 2: alt_qname.append(qn)
        
        # self.ref_qname = ref_qname
        # self.alt_qname = alt_qname
        
        with Pool(processes=self.number_of_processors) as pool:
            results = pool.map_async(self.assign_read, qnames)
            track_progress(results)

            read_results = results.get()
            # read_results.extend(results)

        # ref_qname = [item for sublist in g1_results if sublist is not None for item in sublist]
        # alt_qname = [item for sublist in g2_results if sublist is not None for item in sublist]
        ref_qname = reduce(lambda acc, x: acc + [x[0]] if x else acc, read_results, [])
        ref_qname = [item for sublist in ref_qname for item in sublist]
        alt_qname = reduce(lambda acc, x: acc + [x[1]] if x else acc, read_results, [])
        alt_qname = [item for sublist in alt_qname for item in sublist]

        # return read_results
        haplotagged_qname = defaultdict(list)
        if len(ref_qname)!=0: haplotagged_qname["g1"] = ref_qname
        if len(alt_qname)!=0: haplotagged_qname["g2"] = alt_qname

        tagread_end = datetime.now()
        logging.info("{}:Read assignment is complete!".format(tagread_end.strftime('%Y-%m-%d %H:%M:%S')))
        
        tagread_delta = tagread_end - tagread_start
        tagread_delta = tagread_delta.total_seconds()
        logging.info("Read assignment takes {} seconds!".format(str(tagread_delta)))
        # # self.haplotag_reads = haplotagged_qname

        def output_qname(hp):
            outfile=os.path.basename(self.input_bam_file)
            output_file = os.path.join(self.outdir, "{}.haplotagg_{}.tsv".format(outfile, hp))
            
            # with open(output_file, "w") as out:
            #     for qn in haplotagged_qname[hp]:
            #         out.write(str(qn) + "\n")
            np.savetxt(output_file, haplotagged_qname[hp], fmt = '%s')

        ## Output tagged reads to tsv
        outread_start = datetime.now()
        logging.info("{}.Start writing haplotagged reads to tsv files!".format(outread_start.strftime('%Y-%m-%d %H:%M:%S')))
        
        # try:
        #     self.extract_reads(ref_qname, "g1")
        # except KeyError as e:
        #     logging.error("No reads are haplotagged to g1!")
        #     logging.debug(str(e))

        # try:
        #     self.extract_reads(alt_qname, "g2")
        # except KeyError as e:
        #     logging.error("No reads are haplotagged to g2!")
        #     logging.debug(str(e))

        try:
            output_qname("g1")
        except KeyError as e:
            logging.error("No reads are haplotagged to g1!")
            logging.debug(str(e))

        try:
            output_qname("g2")
        except KeyError as e:
            logging.error("No reads are haplotagged to g2!")
            logging.debug(str(e))
        
        outread_end = datetime.now()
        # logging.info("Writing haplotagged reads to tsv files is complete!")
        logging.info("{}.Writing haplotagged reads to tsv files is complete!".format(outread_end.strftime('%Y-%m-%d %H:%M:%S')))

        outread_delta = outread_end - outread_start
        outread_delta = outread_delta.total_seconds()
        logging.info("Writing reads takes {} seconds!".format(str(outread_delta)))

        # return output_file
        # return final_results

